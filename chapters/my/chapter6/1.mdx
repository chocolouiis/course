# နိဒါန်း[[introduction]]

<CourseFloatingBanner
    chapter={6}
    classNames="absolute z-10 right-0 top-0"
/>

[Chapter 3](/course/chapter3) မှာ၊ သတ်မှတ်ထားတဲ့ task တစ်ခုပေါ်မှာ model တစ်ခုကို fine-tune လုပ်နည်းကို ကျွန်တော်တို့ ကြည့်ခဲ့ပါတယ်။ အဲဒီလိုလုပ်တဲ့အခါ၊ model ကို pretrained လုပ်ခဲ့တဲ့ tokenizer တူတူကို ကျွန်တော်တို့ အသုံးပြုပါတယ်။ — ဒါပေမယ့် model တစ်ခုကို အစကနေ train လုပ်ချင်တဲ့အခါ ဘာလုပ်ရမလဲ။ ဒီလိုအခြေအနေတွေမှာ၊ အခြား domain ဒါမှမဟုတ် language က corpus တစ်ခုပေါ်မှာ pretrained လုပ်ထားတဲ့ tokenizer ကို အသုံးပြုတာက ပုံမှန်အားဖြင့် suboptimal ဖြစ်ပါတယ်။ ဥပမာ၊ English corpus တစ်ခုပေါ်မှာ train ထားတဲ့ tokenizer က Japanese texts corpus ပေါ်မှာ ကောင်းကောင်းအလုပ်လုပ်မှာ မဟုတ်ပါဘူး။ ဘာလို့လဲဆိုတော့ spaces နဲ့ punctuation အသုံးပြုမှုက ဘာသာစကားနှစ်ခုမှာ အလွန်ကွာခြားလို့ပါပဲ။

ဒီအခန်းမှာ၊ စာသား corpus တစ်ခုပေါ်မှာ tokenizer အသစ်တစ်ခုကို ဘယ်လို train လုပ်ရမယ်ဆိုတာ သင်ယူရမှာဖြစ်ပြီး၊ အဲဒါကို language model တစ်ခုကို pretrain လုပ်ဖို့ အသုံးပြုနိုင်ပါလိမ့်မယ်။ ဒါတွေအားလုံးကို [🤗 Tokenizers](https://github.com/huggingface/tokenizers) library ရဲ့ အကူအညီနဲ့ လုပ်ဆောင်သွားမှာပါ။ အဲဒီ library က [🤗 Transformers](https://github.com/huggingface/transformers) library မှာ "fast" tokenizers တွေကို ပံ့ပိုးပေးပါတယ်။ ဒီ library က ပံ့ပိုးပေးတဲ့ features တွေကို အနီးကပ်ကြည့်ရှုပြီး fast tokenizers တွေက "slow" versions တွေနဲ့ ဘယ်လိုကွာခြားလဲဆိုတာ လေ့လာသွားမှာပါ။

ကျွန်တော်တို့ ဖော်ပြမယ့် ခေါင်းစဉ်တွေကတော့-

*   ပေးထားတဲ့ checkpoint တစ်ခုက အသုံးပြုတဲ့ tokenizer နဲ့ ဆင်တူတဲ့ tokenizer အသစ်တစ်ခုကို texts corpus အသစ်တစ်ခုပေါ်မှာ ဘယ်လို train လုပ်ရမလဲ။
*   fast tokenizers တွေရဲ့ သီးခြား features တွေ။
*   ဒီနေ့ခေတ် NLP မှာ အသုံးပြုနေတဲ့ subword tokenization algorithms သုံးခုကြားက ကွာခြားချက်တွေ။
*   🤗 Tokenizers library နဲ့ tokenizer တစ်ခုကို အစကနေ ဘယ်လိုတည်ဆောက်ပြီး data အချို့ပေါ်မှာ train လုပ်ရမလဲ။

ဒီအခန်းမှာ မိတ်ဆက်ပေးမယ့် နည်းလမ်းတွေက [Chapter 7](/course/chapter7/6) မှာ Python source code အတွက် language model တစ်ခု ဖန်တီးတာကို ကြည့်ရှုမယ့် အပိုင်းအတွက် သင့်ကို ပြင်ဆင်ပေးပါလိမ့်မယ်။ ပထမဆုံး tokenizer ကို "train" လုပ်တယ်ဆိုတာ ဘာကိုဆိုလိုသလဲဆိုတာ ကြည့်ခြင်းဖြင့် စတင်ကြရအောင်။

## ဝေါဟာရ ရှင်းလင်းချက် (Glossary)

*   **Fine-tune**: ကြိုတင်လေ့ကျင့်ထားပြီးသား (pre-trained) မော်ဒယ်တစ်ခုကို သီးခြားလုပ်ငန်းတစ်ခု (specific task) အတွက် အနည်းငယ်သော ဒေတာနဲ့ ထပ်မံလေ့ကျင့်ပေးခြင်းကို ဆိုလိုပါတယ်။
*   **Model**: Artificial Intelligence (AI) နယ်ပယ်တွင် အချက်အလက်များကို လေ့လာပြီး ခန့်မှန်းချက်များ ပြုလုပ်ရန် ဒီဇိုင်းထုတ်ထားသော သင်္ချာဆိုင်ရာဖွဲ့စည်းပုံများ။
*   **Tokenizer**: စာသား (သို့မဟုတ် အခြားဒေတာ) ကို AI မော်ဒယ်များ စီမံဆောင်ရွက်နိုင်ရန် tokens တွေအဖြစ် ပိုင်းခြားပေးသည့် ကိရိယာ သို့မဟုတ် လုပ်ငန်းစဉ်။
*   **Pretrained**: Model တစ်ခုကို အကြီးစားဒေတာများဖြင့် အစောပိုင်းကတည်းက လေ့ကျင့်ထားခြင်း။
*   **From Scratch**: Model (သို့မဟုတ် tokenizer) တစ်ခုကို မည်သည့် အစောပိုင်းလေ့ကျင့်မှုမျှ မရှိဘဲ လုံးဝအသစ်ကနေ စတင်တည်ဆောက်ခြင်းနှင့် လေ့ကျင့်ခြင်း။
*   **Corpus**: စာသား (သို့မဟုတ် အခြားဒေတာ) အစုအဝေးကြီးတစ်ခု။
*   **Domain**: သီးခြားနယ်ပယ် (ဥပမာ- ဆေးပညာ domain, ဘဏ္ဍာရေး domain)။
*   **Language**: ဘာသာစကား။
*   **Suboptimal**: အကောင်းဆုံး မဟုတ်ဘဲ စွမ်းဆောင်ရည် နည်းပါးခြင်း။
*   **Punctuation**: စာသားများတွင် အသုံးပြုသော သတ်ပုံအမှတ်အသားများ (ဥပမာ- comma, period, question mark)။
*   **Language Model**: လူသားဘာသာစကား၏ ဖြန့်ဝေမှုကို နားလည်ရန် လေ့ကျင့်ထားသော AI မော်ဒယ်တစ်ခု။ ၎င်းသည် စာသားထုတ်လုပ်ခြင်း၊ ဘာသာပြန်ခြင်း စသည့်လုပ်ငန်းများတွင် အသုံးပြုနိုင်သည်။
*   **🤗 Tokenizers Library**: Rust ဘာသာနဲ့ ရေးသားထားတဲ့ Hugging Face library တစ်ခုဖြစ်ပြီး မြန်ဆန်ထိရောက်တဲ့ tokenization ကို လုပ်ဆောင်ပေးသည်။ 🤗 Transformers library အတွက် "fast" tokenizers တွေကို ပံ့ပိုးပေးသည်။
*   **🤗 Transformers Library**: Hugging Face က ထုတ်လုပ်ထားတဲ့ library တစ်ခုဖြစ်ပြီး Transformer မော်ဒယ်တွေကို အသုံးပြုပြီး Natural Language Processing (NLP), computer vision, audio processing စတဲ့ နယ်ပယ်တွေမှာ အဆင့်မြင့် AI မော်ဒယ်တွေကို တည်ဆောက်ပြီး အသုံးပြုနိုင်စေပါတယ်။
*   **Fast Tokenizers**: Rust ဘာသာစကားဖြင့် အကောင်အထည်ဖော်ထားသော tokenizers များဖြစ်ပြီး Python-based "slow" tokenizers များထက် အလွန်မြန်ဆန်သည်။
*   **Slow Versions (Tokenizers)**: Python ဘာသာစကားဖြင့် အကောင်အထည်ဖော်ထားသော tokenizers များ။
*   **Checkpoint**: မော်ဒယ်၏ weights များနှင့် အခြားဖွဲ့စည်းပုံများ (configuration) ကို သတ်မှတ်ထားသော အချိန်တစ်ခုတွင် သိမ်းဆည်းထားခြင်း။
*   **Subword Tokenization Algorithms**: စကားလုံးများကို သေးငယ်သော subword units (ဥပမာ- word pieces, byte-pair encodings) များအဖြစ် ပိုင်းခြားသော tokenization နည်းလမ်းများ။ ၎င်းသည် vocabulary အရွယ်အစားကို ထိန်းချုပ်ရန်နှင့် out-of-vocabulary (OOV) ပြဿနာများကို ဖြေရှင်းရန် ကူညီပေးသည်။
*   **NLP (Natural Language Processing)**: ကွန်ပျူတာတွေ လူသားဘာသာစကားကို နားလည်၊ အဓိပ္ပာယ်ဖော်ပြီး၊ ဖန်တီးနိုင်အောင် လုပ်ဆောင်ပေးတဲ့ Artificial Intelligence (AI) ရဲ့ နယ်ပယ်ခွဲတစ်ခုပါ။
*   **From Scratch (Tokenizer)**: မည်သည့် ကြိုတင်လေ့ကျင့်မှုမျှ မရှိဘဲ လုံးဝအသစ်ကနေ စတင်တည်ဆောက်ခြင်းနှင့် လေ့ကျင့်ခြင်း (tokenizer အတွက်)။
*   **Python Source Code**: Python programming language ဖြင့် ရေးသားထားသော code များ။