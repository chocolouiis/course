<!-- DISABLE-FRONTMATTER-SECTIONS -->

# အခန်း (၆) ဆိုင်ရာ မေးခွန်းများ[[end-of-chapter-quiz]]

<CourseFloatingBanner
    chapter={6}
    classNames="absolute z-10 right-0 top-0"
/>

ဒီအခန်းမှာ သင်ယူခဲ့တာတွေကို စစ်ဆေးကြည့်ရအောင်။

### ၁။ Tokenizer အသစ်တစ်ခုကို ဘယ်အချိန်မှာ train လုပ်သင့်သလဲ။

<Question
	choices={[
		{
			text: "သင့် dataset က လက်ရှိ pretrained model တစ်ခုက အသုံးပြုတဲ့ dataset နဲ့ ဆင်တူပြီး၊ model အသစ်တစ်ခုကို pretrain လုပ်ချင်တဲ့အခါ။",
			explain: "ဒီကိစ္စမှာ၊ အချိန်နဲ့ compute resources တွေကို ချွေတာဖို့အတွက်၊ pretrained model ရဲ့ tokenizer တူတူကို အသုံးပြုပြီး အဲဒီ model ကို fine-tune လုပ်တာက ပိုကောင်းတဲ့ ရွေးချယ်မှုဖြစ်ပါလိမ့်မယ်။"
		},
		{
			text: "သင့် dataset က လက်ရှိ pretrained model တစ်ခုက အသုံးပြုတဲ့ dataset နဲ့ ဆင်တူပြီး၊ ဒီ pretrained model ကို အသုံးပြုပြီး model အသစ်တစ်ခုကို fine-tune လုပ်ချင်တဲ့အခါ။",
			explain: "pretrained model တစ်ခုကနေ model တစ်ခုကို fine-tune လုပ်ဖို့အတွက်၊ သင်ဟာ အမြဲတမ်း tokenizer တူတူကို အသုံးပြုသင့်ပါတယ်။"
		},
		{
			text: "သင့် dataset က လက်ရှိ pretrained model တစ်ခုက အသုံးပြုတဲ့ dataset နဲ့ ကွဲပြားပြီး၊ model အသစ်တစ်ခုကို pretrain လုပ်ချင်တဲ့အခါ။",
			explain: "မှန်ပါတယ်။ ဒီကိစ္စမှာ tokenizer တူတူကို အသုံးပြုခြင်းက အကျိုးကျေးဇူး မရှိပါဘူး။",
            correct: true
		},
        {
			text: "သင့် dataset က လက်ရှိ pretrained model တစ်ခုက အသုံးပြုတဲ့ dataset နဲ့ ကွဲပြားပြီး၊ ဒီ pretrained model ကို အသုံးပြုပြီး model အသစ်တစ်ခုကို fine-tune လုပ်ချင်တဲ့အခါ။",
			explain: "pretrained model တစ်ခုကနေ model တစ်ခုကို fine-tune လုပ်ဖို့အတွက်၊ သင်ဟာ အမြဲတမ်း tokenizer တူတူကို အသုံးပြုသင့်ပါတယ်။"
		}
	]}
/>

### ၂။ `train_new_from_iterator()` ကို အသုံးပြုတဲ့အခါ list of lists of texts တွေနဲ့ နှိုင်းယှဉ်ရင် generator of lists of texts တွေကို အသုံးပြုခြင်းရဲ့ အကျိုးကျေးဇူးက ဘာလဲ။

<Question
	choices={[
		{
			text: "ဒါက <code>train_new_from_iterator()</code> method က လက်ခံတဲ့ တစ်ခုတည်းသော အမျိုးအစားပါ။",
			explain: "list of lists of texts တွေက generator of lists of texts တွေရဲ့ သီးခြားအမျိုးအစားတစ်ခုဖြစ်တာကြောင့်၊ method က ဒါကိုလည်း လက်ခံပါလိမ့်မယ်။ ထပ်ကြိုးစားပါ။"
		},
		{
			text: "dataset တစ်ခုလုံးကို memory ထဲကို တစ်ပြိုင်နက်တည်း loading လုပ်တာကို ရှောင်ရှားပါလိမ့်မယ်။",
			explain: "မှန်ပါတယ်။ texts တွေရဲ့ batch တစ်ခုစီကို သင် iterate လုပ်တဲ့အခါ memory ကနေ release လုပ်ပါလိမ့်မယ်၊ ပြီးတော့ သင် 🤗 Datasets ကို သင့် texts တွေကို သိမ်းဆည်းဖို့ အသုံးပြုရင် အကျိုးကျေးဇူးက အထူးသဖြင့် မြင်သာပါလိမ့်မယ်။",
			correct: true
		},
		{
			text: "ဒါက 🤗 Tokenizers library ကို multiprocessing အသုံးပြုနိုင်စေပါလိမ့်မယ်။",
			explain: "မဟုတ်ပါဘူး၊ ဒါက ဘယ်လိုပဲဖြစ်ဖြစ် multiprocessing ကို အသုံးပြုပါလိမ့်မယ်။"
		},
        {
			text: "သင် train လုပ်တဲ့ tokenizer က ပိုကောင်းတဲ့ texts တွေကို ထုတ်လုပ်ပါလိမ့်မယ်။",
			explain: "tokenizer က text ကို ထုတ်လုပ်တာ မဟုတ်ပါဘူး -- သင် ဒါကို language model နဲ့ မှားနေတာလား။"
		}
	]}
/>

### ၃။ "Fast" tokenizer တစ်ခုကို အသုံးပြုခြင်းရဲ့ အကျိုးကျေးဇူးတွေက ဘာတွေလဲ။

<Question
	choices={[
		{
			text: "inputs အများအပြားကို batch အလိုက် စုစည်းပြီး လုပ်ဆောင်တဲ့အခါ slow tokenizer ထက် ပိုမြန်မြန် လုပ်ဆောင်နိုင်ပါတယ်။",
			explain: "မှန်ပါတယ်။ Rust မှာ implement လုပ်ထားတဲ့ parallelism ကြောင့်၊ inputs batches တွေပေါ်မှာ ပိုမြန်ပါလိမ့်မယ်။ တခြားဘယ်အကျိုးကျေးဇူးကို သင်စဉ်းစားနိုင်သေးလဲ။",
			correct: true
		},
		{
			text: "Fast tokenizers တွေက ၎င်းတို့ရဲ့ slow counterparts တွေထက် အမြဲတမ်း ပိုမြန်မြန် tokenize လုပ်ပါတယ်။",
			explain: "fast tokenizer က text တစ်ခုတည်း ဒါမှမဟုတ် နည်းနည်းပဲ ပေးတဲ့အခါ ပိုနှေးနိုင်ပါတယ်၊ ဘာလို့လဲဆိုတော့ parallelism ကို အသုံးပြုလို့ မရလို့ပါ။"
		},
		{
			text: "ဒါက padding နဲ့ truncation ကို အသုံးပြုနိုင်ပါတယ်။",
			explain: "မှန်ပါတယ်၊ ဒါပေမယ့် slow tokenizers တွေလည်း အဲဒါကို လုပ်ပါတယ်။"
		},
        {
			text: "ဒါက tokens တွေကို ၎င်းတို့ကို ဖန်တီးခဲ့တဲ့ text span နဲ့ map လုပ်နိုင်စေတဲ့ ထပ်ဆောင်း features တွေ ရှိပါတယ်။",
			explain: "ဟုတ်ပါတယ်။ ဒါတွေကို offset mappings လို့ ခေါ်ပါတယ်။ ဒါပေမယ့် ဒါတစ်ခုတည်းသော အကျိုးကျေးဇူးတော့ မဟုတ်ပါဘူး။",
			correct: true
		}
	]}
/>

### ၄။ `token-classification` pipeline က tokens အများအပြားကို ဖြန့်ကျက်ထားတဲ့ entities တွေကို ဘယ်လို ကိုင်တွယ်ဖြေရှင်းလဲ။

<Question
	choices={[
		{
			text: "တူညီတဲ့ label ရှိတဲ့ entities တွေကို entity တစ်ခုတည်းအဖြစ် ပေါင်းစပ်ပါတယ်။",
			explain: "ဒါက အရာတွေကို နည်းနည်းလေး ရိုးရှင်းလွန်းအောင် ပြောတာပါပဲ။ ထပ်ကြိုးစားပါ။"
		},
		{
			text: "entity ရဲ့ အစအတွက် label တစ်ခုနဲ့ entity ရဲ့ ဆက်လက်ဖြစ်ပေါ်မှုအတွက် label တစ်ခု ရှိပါတယ်။",
			explain: "မှန်ပါတယ်။",
			correct: true
		},
		{
			text: "ပေးထားတဲ့ word တစ်ခုမှာ၊ ပထမဆုံး token မှာ entity ရဲ့ label ရှိနေသရွေ့၊ word တစ်ခုလုံးကို အဲဒီ entity နဲ့ label လုပ်ထားတယ်လို့ သတ်မှတ်ပါတယ်။",
			explain: "ဒါက entities တွေကို ကိုင်တွယ်တဲ့ strategy တစ်ခုပါ။ ဒီနေရာမှာ တခြားဘယ်အဖြေတွေက သက်ဆိုင်လဲ။",
			correct: true
		},
        {
			text: "token တစ်ခုမှာ ပေးထားတဲ့ entity ရဲ့ label ရှိနေတဲ့အခါ၊ အဲဒီနောက်မှာ လိုက်လာတဲ့ တူညီတဲ့ label ရှိတဲ့ တခြား token မှန်သမျှကို entity အသစ်တစ်ခုရဲ့ အစအဖြစ် label မလုပ်ထားသရွေ့ တူညီတဲ့ entity ရဲ့ အစိတ်အပိုင်းအဖြစ် သတ်မှတ်ပါတယ်။",
			explain: "ဒါက entities တွေကို အတူတကွ အုပ်စုဖွဲ့ဖို့ အသုံးအများဆုံး နည်းလမ်းပါ -- ဒါပေမယ့် ဒါတစ်ခုတည်းတော့ အဖြေမှန် မဟုတ်ပါဘူး။",
			correct: true
		}
	]}
/>

### ၅။ `question-answering` pipeline က ရှည်လျားတဲ့ contexts တွေကို ဘယ်လို ကိုင်တွယ်ဖြေရှင်းလဲ။

<Question
	choices={[
		{
			text: "ဒါက တကယ်တမ်း ကိုင်တွယ်ဖြေရှင်းတာ မဟုတ်ပါဘူး၊ ဘာလို့လဲဆိုတော့ model က လက်ခံတဲ့ အမြင့်ဆုံးအရှည်မှာ ရှည်လျားတဲ့ context ကို truncate လုပ်လို့ပါ။",
			explain: "ရှည်လျားတဲ့ contexts တွေကို ကိုင်တွယ်ဖို့ သင်အသုံးပြုနိုင်တဲ့ trick တစ်ခုရှိပါတယ်။ ဒါက ဘာလဲဆိုတာ သင်မှတ်မိလား။"
		},
		{
			text: "ဒါက context ကို အစိတ်အပိုင်းများစွာအဖြစ် ပိုင်းခြားပြီး ရရှိတဲ့ ရလဒ်တွေကို ပျမ်းမျှယူပါတယ်။",
			explain: "မဟုတ်ပါဘူး၊ အဖြေမပါဝင်တဲ့ context အစိတ်အပိုင်းအချို့ရှိနေမှာဖြစ်တာကြောင့် ရလဒ်တွေကို ပျမ်းမျှယူတာ အဓိပ္ပာယ်ရှိမှာ မဟုတ်ပါဘူး။"
		},
		{
			text: "ဒါက context ကို အစိတ်အပိုင်းများစွာအဖြစ် ပိုင်းခြားပြီး (overlap ပါဝင်ပြီး) အစိတ်အပိုင်းတစ်ခုစီမှာ အဖြေအတွက် အမြင့်ဆုံး score ကို ရှာဖွေပါတယ်။",
			explain: "ဒါက မှန်ကန်တဲ့ အဖြေပါ!",
			correct: true
		},
        {
			text: "ဒါက context ကို အစိတ်အပိုင်းများစွာအဖြစ် ပိုင်းခြားပြီး (ထိရောက်မှုအတွက် overlap မပါဘဲ) အစိတ်အပိုင်းတစ်ခုစီမှာ အဖြေအတွက် အမြင့်ဆုံး score ကို ရှာဖွေပါတယ်။",
			explain: "မဟုတ်ပါဘူး၊ အဖြေက အစိတ်အပိုင်းနှစ်ခုကြားမှာ ကွဲသွားနိုင်တဲ့ အခြေအနေကို ရှောင်ရှားဖို့အတွက် အစိတ်အပိုင်းတွေကြားမှာ overlap အချို့ ပါဝင်ပါတယ်။"
		}
	]}
/>

### ၆။ Normalization ဆိုတာ ဘာလဲ။

<Question
	choices={[
		{
			text: "ဒါက tokenizer က ကနဦးအဆင့်တွေမှာ texts တွေပေါ်မှာ လုပ်ဆောင်တဲ့ မည်သည့် သန့်ရှင်းရေးမဆို။",
			explain: "မှန်ပါတယ်။ ဥပမာ- accents တွေ ဒါမှမဟုတ် whitespace တွေ ဖယ်ရှားတာ၊ ဒါမှမဟုတ် inputs တွေကို lowercasing လုပ်တာ ပါဝင်နိုင်ပါတယ်။",
			correct: true
		},
		{
			text: "ဒါက data augmentation technique တစ်ခုဖြစ်ပြီး ရှားပါးတဲ့ words တွေကို ဖယ်ရှားခြင်းဖြင့် text ကို ပိုမို normal ဖြစ်အောင် လုပ်ဆောင်တာပါ။",
			explain: "ဒါက မမှန်ပါဘူး! ထပ်ကြိုးစားပါ။"
		},
		{
			text: "ဒါက tokenizer က special tokens တွေကို ထည့်သွင်းတဲ့ နောက်ဆုံး post-processing အဆင့်ပါ။",
			explain: "အဲဒီအဆင့်ကို ရိုးရှင်းစွာ post-processing လို့ခေါ်ပါတယ်။"
		},
        {
			text: "ဒါက embeddings တွေကို mean 0 နဲ့ standard deviation 1 ဖြစ်အောင်၊ mean ကို နှုတ်ပြီး std နဲ့ ပိုင်းခြင်းဖြင့် လုပ်ဆောင်တာပါ။",
			explain: "အဲဒီလုပ်ငန်းစဉ်ကို computer vision မှာ pixel values တွေပေါ်မှာ အသုံးပြုတဲ့အခါ normalization လို့ အများအားဖြင့် ခေါ်ပါတယ်၊ ဒါပေမယ့် NLP မှာ normalization ရဲ့ အဓိပ္ပာယ် မဟုတ်ပါဘူး။"
		}
	]}
/>

### ၇။ Subword tokenizer အတွက် pre-tokenization ဆိုတာဘာလဲ။

<Question
	choices={[
		{
			text: "ဒါက tokenization မတိုင်မီ အဆင့်ဖြစ်ပြီး data augmentation (random masking လိုမျိုး) ကို အသုံးပြုတာပါ။",
			explain: "မဟုတ်ပါဘူး၊ အဲဒီအဆင့်က preprocessing ရဲ့ အစိတ်အပိုင်းတစ်ခုပါ။"
		},
		{
			text: "ဒါက tokenization မတိုင်မီ အဆင့်ဖြစ်ပြီး လိုအပ်တဲ့ သန့်ရှင်းရေးလုပ်ငန်းတွေကို text ပေါ်မှာ အသုံးပြုတာပါ။",
			explain: "မဟုတ်ပါဘူး၊ ဒါက normalization အဆင့်ပါ။"
		},
		{
			text: "ဒါက tokenizer model ကို အသုံးမပြုမီ အဆင့်ဖြစ်ပြီး input ကို words တွေအဖြစ် ပိုင်းခြားဖို့ပါ။",
			explain: "ဒါက မှန်ကန်တဲ့ အဖြေပါ!",
			correct: true
		},
        {
			text: "ဒါက tokenizer model ကို အသုံးမပြုမီ အဆင့်ဖြစ်ပြီး input ကို tokens တွေအဖြစ် ပိုင်းခြားဖို့ပါ။",
			explain: "မဟုတ်ပါဘူး၊ tokens တွေအဖြစ် ပိုင်းခြားတာက tokenizer model ရဲ့ အလုပ်ပါ။"
		}
	]}
/>

### ၈။ BPE tokenization model နဲ့ သက်ဆိုင်တဲ့ စာကြောင်းတွေကို ရွေးချယ်ပါ။

<Question
	choices={[
		{
			text: "BPE ဟာ small vocabulary ကနေ စတင်ပြီး merge rules တွေကို သင်ယူတဲ့ subword tokenization algorithm တစ်ခုပါ။",
			explain: "ဟုတ်ပါတယ်။",
			correct: true
		},
		{
			text: "BPE ဟာ big vocabulary ကနေ စတင်ပြီး ၎င်းကနေ tokens တွေကို တဖြည်းဖြည်း ဖယ်ရှားတဲ့ subword tokenization algorithm တစ်ခုပါ။",
			explain: "မဟုတ်ပါဘူး၊ ဒါက တခြား tokenization algorithm တစ်ခုက ချဉ်းကပ်တဲ့ နည်းလမ်းပါ။"
		},
		{
			text: "BPE tokenizers တွေက အကြိမ်အများဆုံး ဖြစ်ပေါ်တဲ့ tokens တွဲကို merge လုပ်ခြင်းဖြင့် merge rules တွေကို သင်ယူပါတယ်။",
			explain: "မှန်ပါတယ်!",
			correct: true
		},
		{
			text: "BPE tokenizer က merge rule တစ်ခုကို အကြိမ်များစွာ ဖြစ်ပေါ်ပြီး တစ်ဦးချင်းစီ အစိတ်အပိုင်းတွေက နည်းနည်းပဲ ဖြစ်ပေါ်တဲ့ pairs တွေကို အလေးပေးတဲ့ score တစ်ခုကို အမြင့်ဆုံးဖြစ်စေမယ့် tokens တွဲကို merge လုပ်ခြင်းဖြင့် သင်ယူပါတယ်။",
			explain: "မဟုတ်ပါဘူး၊ ဒါက တခြား tokenization algorithm တစ်ခုက အသုံးချတဲ့ strategy ပါ။"
		},
		{
			text: "BPE က words တွေကို characters တွေအဖြစ် ပိုင်းခြားပြီး merge rules တွေကို အသုံးပြုခြင်းဖြင့် subwords တွေအဖြစ် tokenize လုပ်ပါတယ်။",
			explain: "မှန်ပါတယ်!",
			correct: true
		},
		{
			text: "BPE က words တွေကို vocabulary ထဲမှာရှိတဲ့ word ရဲ့ အစကနေ စတင်တဲ့ အရှည်ဆုံး subword ကို ရှာဖွေပြီး၊ ကျန်တဲ့ text အတွက် လုပ်ငန်းစဉ်ကို ထပ်ခါတလဲလဲ လုပ်ဆောင်ခြင်းဖြင့် subwords တွေအဖြစ် tokenize လုပ်ပါတယ်။",
			explain: "မဟုတ်ပါဘူး၊ ဒါက တခြား tokenization algorithm တစ်ခုရဲ့ လုပ်ဆောင်ပုံပါ။"
		},
	]}
/>

### ၉။ WordPiece tokenization model နဲ့ သက်ဆိုင်တဲ့ စာကြောင်းတွေကို ရွေးချယ်ပါ။

<Question
	choices={[
		{
			text: "WordPiece ဟာ small vocabulary ကနေ စတင်ပြီး merge rules တွေကို သင်ယူတဲ့ subword tokenization algorithm တစ်ခုပါ။",
			explain: "ဟုတ်ပါတယ်။",
			correct: true
		},
		{
			text: "WordPiece ဟာ big vocabulary ကနေ စတင်ပြီး ၎င်းကနေ tokens တွေကို တဖြည်းဖြည်း ဖယ်ရှားတဲ့ subword tokenization algorithm တစ်ခုပါ။",
			explain: "မဟုတ်ပါဘူး၊ ဒါက တခြား tokenization algorithm တစ်ခုက ချဉ်းကပ်တဲ့ နည်းလမ်းပါ။"
		},
		{
			text: "WordPiece tokenizers တွေက အကြိမ်အများဆုံး ဖြစ်ပေါ်တဲ့ tokens တွဲကို merge လုပ်ခြင်းဖြင့် merge rules တွေကို သင်ယူပါတယ်။",
			explain: "မဟုတ်ပါဘူး၊ ဒါက တခြား tokenization algorithm တစ်ခုက အသုံးချတဲ့ strategy ပါ။"
		},
		{
			text: "WordPiece tokenizer က merge rule တစ်ခုကို အကြိမ်များစွာ ဖြစ်ပေါ်ပြီး တစ်ဦးချင်းစီ အစိတ်အပိုင်းတွေက နည်းနည်းပဲ ဖြစ်ပေါ်တဲ့ pairs တွေကို အလေးပေးတဲ့ score တစ်ခုကို အမြင့်ဆုံးဖြစ်စေမယ့် tokens တွဲကို merge လုပ်ခြင်းဖြင့် သင်ယူပါတယ်။",
			explain: "မှန်ပါတယ်!",
			correct: true
		},
		{
			text: "WordPiece က words တွေကို model အရ tokens တွေအဖြစ် အများဆုံး ဖြစ်နိုင်ခြေရှိတဲ့ segmentation ကို ရှာဖွေခြင်းဖြင့် subwords တွေအဖြစ် tokenize လုပ်ပါတယ်။",
			explain: "မဟုတ်ပါဘူး၊ ဒါက တခြား tokenization algorithm ရဲ့ လုပ်ဆောင်ပုံပါ။"
		},
		{
			text: "WordPiece က words တွေကို vocabulary ထဲမှာရှိတဲ့ word ရဲ့ အစကနေ စတင်တဲ့ အရှည်ဆုံး subword ကို ရှာဖွေပြီး၊ ကျန်တဲ့ text အတွက် လုပ်ငန်းစဉ်ကို ထပ်ခါတလဲလဲ လုပ်ဆောင်ခြင်းဖြင့် subwords တွေအဖြစ် tokenize လုပ်ပါတယ်။",
			explain: "ဟုတ်ပါတယ်၊ WordPiece က encoding လုပ်ဆောင်ပုံပါပဲ။",
			correct: true
		},
	]}
/>

### ၁၀။ Unigram tokenization model နဲ့ သက်ဆိုင်တဲ့ စာကြောင်းတွေကို ရွေးချယ်ပါ။

<Question
	choices={[
		{
			text: "Unigram ဟာ small vocabulary ကနေ စတင်ပြီး merge rules တွေကို သင်ယူတဲ့ subword tokenization algorithm တစ်ခုပါ။",
			explain: "မဟုတ်ပါဘူး၊ ဒါက တခြား tokenization algorithm တစ်ခုက ချဉ်းကပ်တဲ့ နည်းလမ်းပါ။"
		},
		{
			text: "Unigram ဟာ big vocabulary ကနေ စတင်ပြီး ၎င်းကနေ tokens တွေကို တဖြည်းဖြည်း ဖယ်ရှားတဲ့ subword tokenization algorithm တစ်ခုပါ။",
			explain: "မှန်ပါတယ်!",
			correct: true
		},
		{
			text: "Unigram က vocabulary ကို whole corpus ပေါ်မှာ တွက်ချက်ထားတဲ့ loss ကို အနည်းဆုံးဖြစ်အောင် လုပ်ဆောင်ခြင်းဖြင့် ၎င်းရဲ့ vocabulary ကို လိုက်လျောညီထွေဖြစ်အောင် ပြောင်းလဲပါတယ်။",
			explain: "မှန်ပါတယ်!",
			correct: true
		},
		{
			text: "Unigram က အကြိမ်အများဆုံး ဖြစ်ပေါ်တဲ့ subwords တွေကို ထိန်းသိမ်းထားခြင်းဖြင့် ၎င်းရဲ့ vocabulary ကို လိုက်လျောညီထွေဖြစ်အောင် ပြောင်းလဲပါတယ်။",
			explain: "မဟုတ်ပါဘူး၊ ဒါက မမှန်ပါဘူး။"
		},
		{
			text: "Unigram က words တွေကို model အရ tokens တွေအဖြစ် အများဆုံး ဖြစ်နိုင်ခြေရှိတဲ့ segmentation ကို ရှာဖွေခြင်းဖြင့် subwords တွေအဖြစ် tokenize လုပ်ပါတယ်။",
			explain: "မှန်ပါတယ်!",
			correct: true
		},
		{
			text: "Unigram က words တွေကို characters တွေအဖြစ် ပိုင်းခြားပြီး၊ merge rules တွေကို အသုံးပြုခြင်းဖြင့် subwords တွေအဖြစ် tokenize လုပ်ပါတယ်။",
			explain: "မဟုတ်ပါဘူး၊ ဒါက တခြား tokenization algorithm ရဲ့ လုပ်ဆောင်ပုံပါ။"
		},
	]}
/>

## ဝေါဟာရ ရှင်းလင်းချက် (Glossary)

*   **Tokenizer**: စာသား (သို့မဟုတ် အခြားဒေတာ) ကို AI မော်ဒယ်များ စီမံဆောင်ရွက်နိုင်ရန် tokens တွေအဖြစ် ပိုင်းခြားပေးသည့် ကိရိယာ သို့မဟုတ် လုပ်ငန်းစဉ်။
*   **Pretrain**: Model တစ်ခုကို အကြီးစားဒေတာများဖြင့် အစောပိုင်းကတည်းက လေ့ကျင့်ထားခြင်း။
*   **Dataset**: AI မော်ဒယ်တွေ လေ့ကျင့်ဖို့အတွက် အသုံးပြုတဲ့ ဒေတာအစုအဝေးတစ်ခုပါ။
*   **Fine-tune**: ကြိုတင်လေ့ကျင့်ထားပြီးသား (pre-trained) မော်ဒယ်တစ်ခုကို သီးခြားလုပ်ငန်းတစ်ခု (specific task) အတွက် အနည်းငယ်သော ဒေတာနဲ့ ထပ်မံလေ့ကျင့်ပေးခြင်းကို ဆိုလိုပါတယ်။
*   **Compute Resources**: ကွန်ပျူတာ၏ တွက်ချက်နိုင်စွမ်း (CPU, GPU, RAM)။
*   **Generator**: Python တွင် iteration လုပ်နိုင်သော object တစ်ခုဖြစ်ပြီး ၎င်းသည် အရာအားလုံးကို memory ထဲသို့ တစ်ပြိုင်နက်တည်း သိမ်းဆည်းမထားဘဲ လိုအပ်သလို တန်ဖိုးများကို ထုတ်ပေးသည်။
*   **`train_new_from_iterator()`**: 🤗 Tokenizers library မှ tokenizer အသစ်တစ်ခုကို iterator (ဥပမာ- generator) မှ data ကို အသုံးပြု၍ လေ့ကျင့်သော method။
*   **Memory**: ကွန်ပျူတာ၏ RAM (Random Access Memory)။
*   **🤗 Datasets Library**: Hugging Face က ထုတ်လုပ်ထားတဲ့ library တစ်ခုဖြစ်ပြီး AI မော်ဒယ်တွေ လေ့ကျင့်ဖို့အတွက် ဒေတာအစုအဝေး (datasets) တွေကို လွယ်လွယ်ကူကူ ဝင်ရောက်ရယူ၊ စီမံခန့်ခွဲပြီး အသုံးပြုနိုင်စေပါတယ်။
*   **🤗 Tokenizers Library**: Rust ဘာသာနဲ့ ရေးသားထားတဲ့ Hugging Face library တစ်ခုဖြစ်ပြီး မြန်ဆန်ထိရောက်တဲ့ tokenization ကို လုပ်ဆောင်ပေးသည်။
*   **Multiprocessing**: ကွန်ပျူတာ၏ processors အများအပြားကို အသုံးပြု၍ လုပ်ငန်းများကို တစ်ပြိုင်နက်တည်း လုပ်ဆောင်ခြင်း။
*   **Language Model**: လူသားဘာသာစကား၏ ဖြန့်ဝေမှုကို နားလည်ရန် လေ့ကျင့်ထားသော AI မော်ဒယ်တစ်ခု။ ၎င်းသည် စာသားထုတ်လုပ်ခြင်း၊ ဘာသာပြန်ခြင်း စသည့်လုပ်ငန်းများတွင် အသုံးပြုနိုင်သည်။
*   **"Fast" Tokenizer**: Rust ဘာသာစကားဖြင့် အကောင်အထည်ဖော်ထားသော tokenizer ဖြစ်ပြီး Python-based "slow" tokenizers များထက် အလွန်မြန်ဆန်သည်။
*   **"Slow" Tokenizer**: Python ဘာသာစကားဖြင့် အကောင်အထည်ဖော်ထားသော tokenizer။
*   **Batch (of inputs)**: မတူညီသော input များစွာကို တစ်ပြိုင်နက်တည်း လုပ်ဆောင်နိုင်ရန် အုပ်စုဖွဲ့ခြင်း။
*   **Parallelism**: လုပ်ငန်းများစွာကို တစ်ပြိုင်နက်တည်း လုပ်ဆောင်ခြင်း။
*   **Rust**: System programming language တစ်ခုဖြစ်ပြီး performance မြင့်မားသော applications များ တည်ဆောက်ရာတွင် အသုံးပြုသည်။
*   **Padding**: input sequences များ၏ အရှည်ကို တူညီစေရန်အတွက် အပို tokens များ ထည့်သွင်းခြင်း။
*   **Truncation**: input sequences များကို သတ်မှတ်ထားသော အရှည်တစ်ခုအထိ ဖြတ်တောက်ခြင်း။
*   **Offset Mappings**: token တစ်ခုစီသည် မူရင်းစာသား၏ မည်သည့်စတင်ခြင်းနှင့် အဆုံးသတ် character index များကြားတွင် ရှိနေသည်ကို ဖော်ပြသော map။
*   **`token-classification` Pipeline**: `pipeline()` function ကို အသုံးပြု၍ token classification task ကို လုပ်ဆောင်ရန် တည်ဆောက်ထားသော pipeline။
*   **Entities**: Named Entity Recognition (NER) တွင် ဖော်ထုတ်ရမည့် အရာများ (ဥပမာ- လူပုဂ္ဂိုလ်၊ နေရာဒေသ၊ အဖွဲ့အစည်း)။
*   **Label**: Classification task တစ်ခုတွင် data point တစ်ခုအား သတ်မှတ်ထားသော အမျိုးအစား။
*   **`B-XXX` Label**: Named Entity Recognition (NER) တွင် entity အမျိုးအစား `XXX` ၏ စတင်ခြင်း token ကို ကိုယ်စားပြုသော label ("Beginning" of entity)။
*   **`I-XXX` Label**: Named Entity Recognition (NER) တွင် entity အမျိုးအစား `XXX` ၏ အတွင်းပိုင်း token ကို ကိုယ်စားပြုသော label ("Inside" entity)။
*   **`question-answering` Pipeline**: `pipeline()` function ကို အသုံးပြု၍ question answering task ကို လုပ်ဆောင်ရန် တည်ဆောက်ထားသော pipeline။
*   **Contexts**: Question answering task တွင် မေးခွန်းအတွက် အဖြေပါဝင်နိုင်သည့် စာသားအပိုဒ်။
*   **Truncate (Context)**: ရှည်လျားသော context ကို model ၏ maximum length အထိ ဖြတ်တောက်ခြင်း။
*   **Overlap (Context)**: ရှည်လျားသော context ကို အပိုင်းပိုင်းပိုင်းဖြတ်ရာတွင် အပိုင်းများကြားတွင် တူညီသော စာသားအချို့ ထပ်နေခြင်း။
*   **Normalization**: စာသားကို သန့်ရှင်းရေးလုပ်ခြင်း (ဥပမာ- needless whitespace ဖယ်ရှားခြင်း၊ lowercasing, accents ဖယ်ရှားခြင်း)။
*   **Data Augmentation**: datasets ၏ အရွယ်အစားနှင့် မတူကွဲပြားမှုကို တိုးမြှင့်ရန်အတွက် လက်ရှိဒေတာကို ပြောင်းလဲခြင်း သို့မဟုတ် ဒေတာအသစ်များ ဖန်တီးခြင်းနည်းလမ်း။
*   **Rare Words**: corpus ထဲတွင် အကြိမ်ရေနည်းပါးစွာသာ ပေါ်ပေါက်သော စကားလုံးများ။
*   **Post-processing**: Model ၏ output များကို နောက်ဆုံးအသုံးပြုမှုအတွက် ပြင်ဆင်ခြင်း လုပ်ငန်းစဉ်။
*   **Embeddings**: စကားလုံးများ၊ စာကြောင်းများ သို့မဟုတ် အခြားဒေတာများကို ဂဏန်းဆိုင်ရာ vector များအဖြစ် ကိုယ်စားပြုခြင်း။
*   **Mean (Average)**: ပျမ်းမျှတန်ဖိုး။
*   **Standard Deviation (Std)**: data points များသည် mean (ပျမ်းမျှ) မှ မည်မျှကွာဝေးနေသည်ကို တိုင်းတာသော သင်္ချာဆိုင်ရာတန်ဖိုး။
*   **Pixel Values**: ပုံရိပ်တစ်ခုရှိ pixel တစ်ခုစီ၏ အရောင် သို့မဟုတ် အလင်းအမှောင် တန်ဖိုးများ။
*   **Computer Vision**: ကွန်ပျူတာများကို ပုံရိပ်များ သို့မဟုတ် ဗီဒီယိုများမှ အချက်အလက်များ နားလည်စေရန် သင်ကြားပေးခြင်း။
*   **Pre-tokenization**: Subword tokenization မလုပ်ဆောင်မီ စာသားကို ပိုမိုသေးငယ်သော entities (ဥပမာ- words) အဖြစ် အကြိုပိုင်းခြားခြင်း။
*   **Subword Tokenizer**: စကားလုံးများကို သေးငယ်သော subword units (ဥပမာ- word pieces, byte-pair encodings) များအဖြစ် ပိုင်းခြားသော tokenizer။
*   **Masking (Random Masking)**: data augmentation technique တစ်ခုဖြစ်ပြီး input data အစိတ်အပိုင်းအချို့ကို ကျပန်းဖုံးကွယ်ထားခြင်း။
*   **Tokenizer Model**: Tokenization လုပ်ငန်းစဉ်ကို လုပ်ဆောင်ပေးသော model။
*   **BPE (Byte-Pair Encoding)**: Subword tokenization algorithm တစ်မျိုး။ small vocabulary မှ စတင်ပြီး အများဆုံးဖြစ်ပေါ်သော tokens တွဲများကို merge လုပ်ခြင်းဖြင့် merge rules များကို သင်ယူသည်။ words များကို characters များအဖြစ် ပိုင်းခြားပြီး merge rules များကို အသုံးပြု၍ subwords များအဖြစ် tokenize လုပ်သည်။
*   **Vocabulary**: tokenizer သို့မဟုတ် model တစ်ခုက သိရှိနားလည်ပြီး ကိုင်တွယ်နိုင်သော ထူးခြားသည့် tokens များ စုစုပေါင်း။
*   **Merge Rules**: BPE နှင့် WordPiece algorithm များတွင် tokens များကို ပေါင်းစပ်ရန် သင်ယူထားသော စည်းမျဉ်းများ။
*   **WordPiece**: Subword tokenization algorithm တစ်မျိုး။ small vocabulary မှ စတင်ပြီး အကြိမ်များစွာ ဖြစ်ပေါ်ပြီး တစ်ဦးချင်းစီ အစိတ်အပိုင်းတွေက နည်းနည်းပဲ ဖြစ်ပေါ်တဲ့ pairs တွေကို အလေးပေးတဲ့ score တစ်ခုကို အမြင့်ဆုံးဖြစ်စေမယ့် tokens တွဲကို merge လုပ်ခြင်းဖြင့် merge rules များကို သင်ယူသည်။ words များကို vocabulary ထဲမှာရှိတဲ့ word ရဲ့ အစကနေ စတင်တဲ့ အရှည်ဆုံး subword ကို ရှာဖွေပြီး၊ ကျန်တဲ့ text အတွက် လုပ်ငန်းစဉ်ကို ထပ်ခါတလဲလဲ လုပ်ဆောင်ခြင်းဖြင့် subwords တွေအဖြစ် tokenize လုပ်သည်။
*   **Segmentation**: စာသားတစ်ခုကို သေးငယ်သော အစိတ်အပိုင်းများ (ဥပမာ- tokens) အဖြစ် ပိုင်းခြားခြင်း။
*   **Unigram**: Subword tokenization algorithm တစ်မျိုး။ big vocabulary မှ စတင်ပြီး whole corpus ပေါ်မှာ တွက်ချက်ထားတဲ့ loss ကို အနည်းဆုံးဖြစ်စေမယ့် tokens တွေကို ဖယ်ရှားခြင်းဖြင့် vocabulary ကို လိုက်လျောညီထွေဖြစ်အောင် ပြောင်းလဲသည်။ words တွေကို model အရ tokens တွေအဖြစ် အများဆုံး ဖြစ်နိုင်ခြေရှိတဲ့ segmentation ကို ရှာဖွေခြင်းဖြင့် subwords တွေအဖြစ် tokenize လုပ်သည်။
*   **Loss (Corpus Loss)**: Model ၏ ခန့်မှန်းချက်များနှင့် အမှန်တကယ် labels များကြား ကွာခြားမှုကို whole corpus တစ်ခုလုံးအတွက် တိုင်းတာသော တန်ဖိုး။