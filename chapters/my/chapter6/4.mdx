<FrameworkSwitchCourse {fw} />

# Normalization နှင့် Pre-tokenization[[normalization-and-pre-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb"},
]} />

Transformer models တွေနဲ့ အသုံးပြုတဲ့ အသုံးအများဆုံး subword tokenization algorithms သုံးခု (Byte-Pair Encoding [BPE]၊ WordPiece နဲ့ Unigram) ကို နက်နက်နဲနဲ လေ့လာမဝင်မီ၊ tokenizer တစ်ခုစီက text ကို ဘယ်လို preprocessing လုပ်လဲဆိုတာ အရင်ဆုံး ကြည့်ရပါမယ်။ tokenization pipeline ရဲ့ အဆင့်တွေကို မြင့်မားသောအဆင့် (high-level) overview တစ်ခုကတော့ အောက်ပါအတိုင်းပါ။

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="The tokenization pipeline.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="The tokenization pipeline.">
</div>

text တစ်ခုကို (၎င်း၏ model အရ) subtokens အဖြစ် ပိုင်းခြားခြင်းမပြုမီ၊ tokenizer သည် အဆင့်နှစ်ဆင့်ကို လုပ်ဆောင်ပါတယ်- _normalization_ နဲ့ _pre-tokenization_ တို့ ဖြစ်ပါတယ်။

## Normalization[[normalization]]

<Youtube id="4IIC2jI9CaU"/>

normalization အဆင့်မှာ မလိုအပ်တဲ့ whitespace တွေ ဖယ်ရှားတာ၊ lowercasing လုပ်တာ၊ နဲ့/သို့မဟုတ် accents တွေ ဖယ်ရှားတာလိုမျိုး အထွေထွေသန့်ရှင်းရေးတွေ ပါဝင်ပါတယ်။ သင် [Unicode normalization](http://www.unicode.org/reports/tr15/) (NFC ဒါမှမဟုတ် NFKC လိုမျိုး) နဲ့ ရင်းနှီးတယ်ဆိုရင်၊ ဒါကလည်း tokenizer က အသုံးချနိုင်တဲ့ အရာတစ်ခုပါပဲ။

🤗 Transformers `tokenizer` မှာ `backend_tokenizer` လို့ခေါ်တဲ့ attribute တစ်ခုရှိပြီး အောက်ခံ tokenizer ကို 🤗 Tokenizers library ကနေ ဝင်ရောက်ကြည့်ရှုနိုင်စေပါတယ်။

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
print(type(tokenizer.backend_tokenizer))
```

```python out
<class 'tokenizers.Tokenizer'>
```

`tokenizer` object ရဲ့ `normalizer` attribute မှာ `normalize_str()` method တစ်ခုရှိပြီး normalization ကို ဘယ်လိုလုပ်ဆောင်လဲဆိုတာ ကြည့်ဖို့ ကျွန်တော်တို့ အသုံးပြုနိုင်ပါတယ်။

```py
print(tokenizer.backend_tokenizer.normalizer.normalize_str("Héllò hôw are ü?"))
```

```python out
'hello how are u?'
```

ဒီဥပမာမှာ၊ ကျွန်တော်တို့ `bert-base-uncased` checkpoint ကို ရွေးချယ်ခဲ့တာကြောင့်၊ normalization က lowercasing လုပ်ပြီး accents တွေကို ဖယ်ရှားခဲ့ပါတယ်။

> [!TIP]
> ✏️ **စမ်းသပ်ကြည့်ပါ။** `bert-base-cased` checkpoint ကနေ tokenizer တစ်ခုကို load လုပ်ပြီး ဥပမာတူတူကို ၎င်းဆီ ပေးပို့ပါ။ cased နဲ့ uncased versions တွေကြားက အဓိကကွာခြားချက်တွေက ဘာတွေလဲ။

## Pre-tokenization[[pre-tokenization]]

<Youtube id="grlLV8AIXug"/>

နောက်အပိုင်းတွေမှာ ကျွန်တော်တို့ တွေ့ရမယ့်အတိုင်း၊ tokenizer တစ်ခုကို raw text တစ်ခုတည်းနဲ့ train လုပ်လို့ မရပါဘူး။ အဲဒီအစား၊ ကျွန်တော်တို့ဟာ texts တွေကို words တွေလို သေးငယ်တဲ့ entities တွေအဖြစ် အရင်ဆုံး ပိုင်းခြားဖို့ လိုအပ်ပါတယ်။ အဲဒီနေရာမှာ pre-tokenization အဆင့်က ပါဝင်လာပါတယ်။ [Chapter 2](/course/chapter2) မှာ ကျွန်တော်တို့ တွေ့ခဲ့ရတဲ့အတိုင်း၊ word-based tokenizer တစ်ခုက raw text တစ်ခုကို whitespace နဲ့ punctuation တွေပေါ်မှာ ရိုးရှင်းစွာ words တွေအဖြစ် ပိုင်းခြားနိုင်ပါတယ်။ အဲဒီ words တွေဟာ tokenizer က ၎င်းရဲ့ training လုပ်နေစဉ်အတွင်း သင်ယူနိုင်တဲ့ subtokens တွေရဲ့ boundary တွေ ဖြစ်ပါလိမ့်မယ်။

fast tokenizer တစ်ခုက pre-tokenization ကို ဘယ်လိုလုပ်ဆောင်လဲဆိုတာ ကြည့်ဖို့၊ `tokenizer` object ရဲ့ `pre_tokenizer` attribute ရဲ့ `pre_tokenize_str()` method ကို ကျွန်တော်တို့ အသုံးပြုနိုင်ပါတယ်။

```py
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python out
[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]
```

tokenizer က offsets တွေကို အရင်ကတည်းက ခြေရာခံထားတာကို သတိပြုပါ။ ဒါက ယခင်အပိုင်းမှာ ကျွန်တော်တို့ အသုံးပြုခဲ့တဲ့ offset mapping ကို ဘယ်လိုပေးနိုင်လဲဆိုတာပါပဲ။ ဒီနေရာမှာ tokenizer က spaces နှစ်ခုကို လျစ်လျူရှုပြီး တစ်ခုတည်းနဲ့ အစားထိုးပါတယ်၊ ဒါပေမယ့် `are` နဲ့ `you` ကြားက offset ကတော့ အဲဒါကို ထည့်သွင်းစဉ်းစားဖို့ ခုန်ကျော်သွားပါတယ်။

ကျွန်တော်တို့ BERT tokenizer ကို အသုံးပြုနေတာကြောင့်၊ pre-tokenization မှာ whitespace နဲ့ punctuation တွေပေါ်မှာ ပိုင်းခြားတာ ပါဝင်ပါတယ်။ တခြား tokenizers တွေမှာ ဒီအဆင့်အတွက် မတူညီတဲ့ စည်းမျဉ်းတွေ ရှိနိုင်ပါတယ်။ ဥပမာ၊ ကျွန်တော်တို့ GPT-2 tokenizer ကို အသုံးပြုမယ်ဆိုရင်...

```py
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

ဒါကလည်း whitespace နဲ့ punctuation တွေပေါ်မှာ ပိုင်းခြားပါလိမ့်မယ်၊ ဒါပေမယ့် spaces တွေကို ထိန်းသိမ်းထားပြီး ၎င်းတို့ကို `Ġ` symbol နဲ့ အစားထိုးပါလိမ့်မယ်။ ဒါက tokens တွေကို decode လုပ်ရင် original spaces တွေကို ပြန်လည်ရယူနိုင်စေပါတယ်။

```python out
[('Hello', (0, 5)), (',', (5, 6)), ('Ġhow', (6, 10)), ('Ġare', (10, 14)), ('Ġ', (14, 15)), ('Ġyou', (15, 19)),
 ('?', (19, 20))]
```

BERT tokenizer နဲ့ မတူဘဲ၊ ဒီ tokenizer က double space ကို လျစ်လျူရှုတာ မရှိဘူးဆိုတာလည်း သတိပြုပါ။

နောက်ဆုံး ဥပမာတစ်ခုအနေနဲ့၊ SentencePiece algorithm ပေါ်မှာ အခြေခံထားတဲ့ T5 tokenizer ကို ကြည့်ရအောင်။

```py
tokenizer = AutoTokenizer.from_pretrained("t5-small")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python out
[(' Hello,', (0, 6)), (' how', (7, 10)), (' are', (11, 14)), (' you?', (16, 20))]
```

GPT-2 tokenizer နဲ့ ဆင်တူစွာ၊ ဒါက spaces တွေကို ထိန်းသိမ်းထားပြီး ၎င်းတို့ကို သီးခြား token တစ်ခု (`_`) နဲ့ အစားထိုးပါတယ်။ ဒါပေမယ့် T5 tokenizer က whitespace တွေပေါ်မှာပဲ ပိုင်းခြားပြီး punctuation တွေပေါ်မှာ မဟုတ်ပါဘူး။ ဒါ့အပြင် sentence အစမှာ ( `Hello` မတိုင်ခင်) default အားဖြင့် space တစ်ခု ထည့်ထားပြီး `are` နဲ့ `you` ကြားက double space ကို လျစ်လျူရှုခဲ့တာကိုလည်း သတိပြုပါ။

ကွဲပြားခြားနားတဲ့ tokenizers အချို့က text ကို ဘယ်လို process လုပ်လဲဆိုတာ အနည်းငယ် မြင်တွေ့ခဲ့ရပြီဆိုတော့၊ အောက်ခံ algorithms တွေကိုယ်တိုင် လေ့လာကြည့်နိုင်ပါပြီ။ ကျွန်တော်တို့ဟာ အကျယ်တဝင့် အသုံးဝင်တဲ့ SentencePiece ကို အမြန်ကြည့်ခြင်းဖြင့် စတင်ပါမယ်။ ထို့နောက်၊ နောက်ထပ် သုံးပိုင်းမှာ၊ subword tokenization အတွက် အဓိက algorithms သုံးခု ဘယ်လိုအလုပ်လုပ်လဲဆိုတာကို စစ်ဆေးကြည့်ပါမယ်။

## SentencePiece[[sentencepiece]]

[SentencePiece](https://github.com/google/sentencepiece) ဟာ text ကို preprocessing လုပ်ဖို့အတွက် tokenization algorithm တစ်ခုဖြစ်ပြီး နောက်ထပ် သုံးအပိုင်းမှာ ကျွန်တော်တို့ မြင်တွေ့ရမယ့် models တွေထဲက ဘယ်တစ်ခုနဲ့မဆို အသုံးပြုနိုင်ပါတယ်။ ဒါက text ကို Unicode characters sequence တစ်ခုအဖြစ် သတ်မှတ်ပြီး၊ spaces တွေကို ` ` ဆိုတဲ့ special character တစ်ခုနဲ့ အစားထိုးပါတယ်။ Unigram algorithm ( [အပိုင်း ၇](/course/chapter6/7) ကို ကြည့်ပါ) နဲ့ ပေါင်းစပ်အသုံးပြုတဲ့အခါ၊ pre-tokenization အဆင့်တောင် မလိုအပ်ပါဘူး။ ဒါက space character ကို အသုံးမပြုတဲ့ ဘာသာစကားတွေ (ဥပမာ- တရုတ် ဒါမှမဟုတ် ဂျပန်) အတွက် အလွန်အသုံးဝင်ပါတယ်။

SentencePiece ရဲ့ အခြားအဓိက feature ကတော့ *reversible tokenization* ပါ၊ spaces တွေကို သီးခြားစီ စီမံဆောင်ရွက်မှု မရှိတဲ့အတွက်၊ tokens တွေကို decode လုပ်တာက ၎င်းတို့ကို concatenate လုပ်ပြီး `_` တွေကို spaces တွေနဲ့ အစားထိုးခြင်းဖြင့် ရိုးရှင်းစွာ လုပ်ဆောင်ပါတယ်။ — ဒါက normalized text ကို ရရှိစေပါတယ်။ ကျွန်တော်တို့ အရင်က တွေ့ခဲ့ရတဲ့အတိုင်း၊ BERT tokenizer က ထပ်နေတဲ့ spaces တွေကို ဖယ်ရှားတာကြောင့်၊ ၎င်းရဲ့ tokenization က reversible မဟုတ်ပါဘူး။

## Algorithm Overview[[algorithm-overview]]

အောက်ပါအပိုင်းတွေမှာ၊ အဓိက subword tokenization algorithms သုံးခုဖြစ်တဲ့ BPE (GPT-2 နဲ့ အခြား models တွေက အသုံးပြု)၊ WordPiece (BERT ကဲ့သို့သော models တွေက အသုံးပြု) နဲ့ Unigram (T5 နဲ့ အခြား models တွေက အသုံးပြု) တွေကို နက်နက်နဲနဲ လေ့လာသွားပါမယ်။ မစတင်မီ၊ ၎င်းတို့တစ်ခုချင်းစီ ဘယ်လိုအလုပ်လုပ်လဲဆိုတာကို အမြန် overview တစ်ခု ပေးလိုက်ပါတယ်။ ဒီဇယားက အခုထိ သင့်အတွက် အဓိပ္ပာယ်မရှိသေးရင် နောက်အပိုင်းတစ်ခုစီကို ဖတ်ပြီးနောက် ဒီဇယားကို ပြန်ကြည့်ဖို့ မတွန့်ဆုတ်ပါနဲ့။

Model | BPE | WordPiece | Unigram
:----:|:---:|:---------:|:------:
Training | small vocabulary ကနေ စတင်ပြီး tokens တွေကို merge လုပ်ဖို့ စည်းမျဉ်းတွေ သင်ယူ | small vocabulary ကနေ စတင်ပြီး tokens တွေကို merge လုပ်ဖို့ စည်းမျဉ်းတွေ သင်ယူ | large vocabulary ကနေ စတင်ပြီး tokens တွေကို ဖယ်ရှားဖို့ စည်းမျဉ်းတွေ သင်ယူ
Training step | အများဆုံး common pair နဲ့ ကိုက်ညီတဲ့ tokens တွေကို merge လုပ် | pair ရဲ့ frequency ပေါ် အခြေခံပြီး best score နဲ့ ကိုက်ညီတဲ့ tokens တွေကို merge လုပ်၊ တစ်ခုချင်းစီ token က less frequent ဖြစ်တဲ့ pairs တွေကို အလေးထား | whole corpus ပေါ်မှာ တွက်ချက်ထားတဲ့ loss ကို အနည်းဆုံးဖြစ်စေမယ့် vocabulary ထဲက tokens အားလုံးကို ဖယ်ရှား
Learns | Merge rules တွေနဲ့ vocabulary | Vocabulary တစ်ခုတည်း | token တစ်ခုစီအတွက် score ပါဝင်တဲ့ vocabulary
Encoding | word ကို characters တွေအဖြစ် ပိုင်းခြားပြီး training လုပ်နေစဉ် သင်ယူခဲ့တဲ့ merges တွေကို အသုံးပြု | vocabulary ထဲမှာရှိတဲ့ word ရဲ့ အစကနေ စတင်တဲ့ အရှည်ဆုံး subword ကို ရှာဖွေ၊ ပြီးတော့ word ရဲ့ ကျန်တာအတွက် အတူတူလုပ် | training လုပ်နေစဉ် သင်ယူခဲ့တဲ့ scores တွေကို အသုံးပြုပြီး tokens တွေအဖြစ် split လုပ်ဖို့ အများဆုံး ဖြစ်နိုင်ခြေရှိတာကို ရှာဖွေ

အခု BPE ကို စတင်လေ့လာကြရအောင်!

## ဝေါဟာရ ရှင်းလင်းချက် (Glossary)

*   **Subword Tokenization Algorithms**: စကားလုံးများကို သေးငယ်သော subword units (ဥပမာ- word pieces, byte-pair encodings) များအဖြစ် ပိုင်းခြားသော tokenization နည်းလမ်းများ။ ၎င်းသည် vocabulary အရွယ်အစားကို ထိန်းချုပ်ရန်နှင့် out-of-vocabulary (OOV) ပြဿနာများကို ဖြေရှင်းရန် ကူညီပေးသည်။
*   **Transformer Models**: Natural Language Processing (NLP) မှာ အောင်မြင်မှုများစွာရရှိခဲ့တဲ့ deep learning architecture တစ်မျိုးပါ။
*   **Byte-Pair Encoding (BPE)**: Subword tokenization algorithm တစ်မျိုး။
*   **WordPiece**: Subword tokenization algorithm တစ်မျိုး။
*   **Unigram**: Subword tokenization algorithm တစ်မျိုး။
*   **Preprocessing**: ဒေတာများကို model က နားလည်ပြီး လုပ်ဆောင်နိုင်တဲ့ ပုံစံအဖြစ် ပြောင်းလဲပြင်ဆင်ခြင်း လုပ်ငန်းစဉ်။
*   **Tokenization Pipeline**: စာသားကို AI မော်ဒယ်များ လုပ်ဆောင်နိုင်သော ဂဏန်းဆိုင်ရာ ကိုယ်စားပြုမှုအဖြစ် ပြောင်းလဲရန် လိုအပ်သော အဆင့်များ (ဥပမာ- normalization, pre-tokenization, tokenization, post-processing)။
*   **Normalization**: စာသားကို သန့်ရှင်းရေးလုပ်ခြင်း (ဥပမာ- needless whitespace ဖယ်ရှားခြင်း၊ lowercasing, accents ဖယ်ရှားခြင်း)။
*   **Pre-tokenization**: Subword tokenization မလုပ်ဆောင်မီ စာသားကို ပိုမိုသေးငယ်သော entities (ဥပမာ- words) အဖြစ် အကြိုပိုင်းခြားခြင်း။
*   **Whitespace**: စာသားများကြားရှိ နေရာလွတ်များ (space, tab, newline)။
*   **Lowercasing**: စာလုံးများကို အသေးစာလုံးများအဖြစ် ပြောင်းလဲခြင်း။
*   **Accents**: စာလုံးများပေါ်တွင် တင်ရှိသော အသံထွက် သင်္ကေတများ (ဥပမာ- é, ü)။
*   **Unicode Normalization**: Unicode character များကို တူညီသော စံနှုန်းတစ်ခုအဖြစ် ပြောင်းလဲခြင်း။
*   **NFC/NFKC**: Unicode normalization forms များ။
*   **🤗 Transformers `tokenizer`**: Hugging Face Transformers library မှ tokenizer object။
*   **`backend_tokenizer` Attribute**: `tokenizer` object မှ underlying tokenizer (🤗 Tokenizers library က) ကို ဝင်ရောက်ကြည့်ရှုနိုင်စေသော attribute။
*   **🤗 Tokenizers Library**: Rust ဘာသာနဲ့ ရေးသားထားတဲ့ Hugging Face library တစ်ခုဖြစ်ပြီး မြန်ဆန်ထိရောက်တဲ့ tokenization ကို လုပ်ဆောင်ပေးသည်။
*   **`tokenizers.Tokenizer`**: 🤗 Tokenizers library မှ base tokenizer class။
*   **`normalizer` Attribute**: `tokenizer` object မှ normalization ကို လုပ်ဆောင်ပေးသော object။
*   **`normalize_str()` Method**: Normalizer object မှ string တစ်ခုကို normalize လုပ်သော method။
*   **`bert-base-uncased`**: BERT model ၏ base version အတွက် checkpoint identifier (uncased version)။
*   **Cased/Uncased Versions**: cased version က စာလုံးအကြီးအသေးကို ခွဲခြားပြီး uncased version က စာလုံးအကြီးအသေးကို မခွဲခြားပါ။
*   **Raw Text**: မည်သည့် preprocessing မျှ မလုပ်ဆောင်ရသေးသော စာသား။
*   **Words**: စာသားကို ပိုင်းခြားထားသော အခြေခံယူနစ်များ။
*   **Boundaries**: အရာနှစ်ခုကြား ပိုင်းခြားထားသော အစွန်းများ။
*   **Subtokens**: Subword tokenization ပြီးနောက် ရရှိသော သေးငယ်သော tokens များ။
*   **Word-based Tokenizer**: စကားလုံးများကို အခြေခံ၍ စာသားကို ပိုင်းခြားသော tokenizer။
*   **Punctuation**: စာသားများတွင် အသုံးပြုသော သတ်ပုံအမှတ်အသားများ (ဥပမာ- comma, period, question mark)။
*   **`pre_tokenizer` Attribute**: `tokenizer` object မှ pre-tokenization ကို လုပ်ဆောင်ပေးသော object။
*   **`pre_tokenize_str()` Method**: Pre-tokenizer object မှ string တစ်ခုကို pre-tokenize လုပ်သော method။
*   **Offsets**: token တစ်ခုစီသည် မူရင်းစာသား၏ မည်သည့်စတင်ခြင်းနှင့် အဆုံးသတ် character index များကြားတွင် ရှိနေသည်ကို ဖော်ပြသော map။
*   **Offset Mapping**: token တစ်ခုစီသည် မူရင်းစာသား၏ မည်သည့်စတင်ခြင်းနှင့် အဆုံးသတ် character index များကြားတွင် ရှိနေသည်ကို ဖော်ပြသော map။
*   **BERT Tokenizer**: BERT model အတွက် အသုံးပြုသော tokenizer။
*   **GPT-2 Tokenizer**: GPT-2 model အတွက် အသုံးပြုသော tokenizer။
*   **`gpt2`**: GPT-2 model ၏ identifier။
*   **`Ġ` Symbol**: GPT-2 tokenizer တွင် space ကို ကိုယ်စားပြုသော symbol။
*   **Decode Tokens**: tokens များကို မူရင်း text အဖြစ် ပြန်ပြောင်းခြင်း။
*   **T5 Tokenizer**: T5 model အတွက် အသုံးပြုသော tokenizer။
*   **`t5-small`**: T5 model ၏ small version အတွက် identifier။
*   **Reversible Tokenization**: tokens များကို decode လုပ်တဲ့အခါ မူရင်း text (သို့မဟုတ် normalized text) ကို ပြန်လည်ရယူနိုင်သော tokenization အမျိုးအစား။
*   **Normalized Text**: Normalization လုပ်ထားသော စာသား။
*   **Vocabulary**: tokenizer သို့မဟုတ် model တစ်ခုက သိရှိနားလည်ပြီး ကိုင်တွယ်နိုင်သော ထူးခြားသည့် tokens များ စုစုပေါင်း။
*   **Merge Rules**: BPE နှင့် WordPiece algorithm များတွင် tokens များကို ပေါင်းစပ်ရန် သင်ယူထားသော စည်းမျဉ်းများ။
*   **Corpus (Training)**: Model သို့မဟုတ် tokenizer ကို လေ့ကျင့်ရန် အသုံးပြုသော စာသားအစုအဝေးကြီး။
*   **Score (Unigram)**: Unigram algorithm တွင် token တစ်ခုစီနှင့် ဆက်စပ်နေသော တန်ဖိုး။
*   **Loss**: Model ၏ ခန့်မှန်းချက်များနှင့် အမှန်တကယ် labels များကြား ကွာခြားမှုကို တိုင်းတာသော တန်ဖိုး။